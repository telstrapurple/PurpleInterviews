<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<link rel="icon" href="/favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<title>Purple Interviews - Data</title>

		

		<link rel="modulepreload" href="/_app/start-6a8dc90b.js">
		<link rel="modulepreload" href="/_app/chunks/vendor-2e27399c.js">
		<link rel="modulepreload" href="/_app/chunks/paths-28a87002.js">
		<link rel="modulepreload" href="/_app/pages/__layout.svelte-f0d5aa20.js">
		<link rel="modulepreload" href="/_app/pages/[slug].svelte-72899930.js">
		<link rel="stylesheet" href="/_app/assets/start-464e9d0a.css">
		<link rel="stylesheet" href="/_app/assets/pages/__layout.svelte-58634277.css">

		<script type="module">
			import { start } from "/_app/start-6a8dc90b.js";
			start({
				target: document.querySelector("#svelte"),
				paths: {"base":"","assets":""},
				session: {},
				host: location.host,
				route: true,
				spa: false,
				trailing_slash: "never",
				hydrate: {
					status: 200,
					error: null,
					nodes: [
						import("/_app/pages/__layout.svelte-f0d5aa20.js"),
						import("/_app/pages/[slug].svelte-72899930.js")
					],
					page: {
						host: location.host, // TODO this is redundant
						path: "/data",
						query: new URLSearchParams(""),
						params: {"slug":"data"}
					}
				}
			});
		</script>
	</head>
	<body>
		<div id="svelte">


<div><div class="filter drop-shadow-md bg-white"><nav class="border-t-2 border-solid border-red"><a href="/"><img src="/tplogo-red.svg" alt="Telstra Purple" class="px-12 lg:px-32 py-2 lg:py-4"></a></nav></div>
	<div class="px-12 lg:px-32 pt-6 lg:pt-24 pb-3 lg:pb-12"><div class="max-w-screen-xl flex flex-col"><div class="flex flex-col lg:flex-row"><div class="flex flex-col"><h1 class="pb-3 lg:pb-12">Purple Interviews</h1>
				<p class="max-w-prose">We&#39;re pleased to invite you to the testing stage of our Consultant roles at Telstra
					Purple. On this page you&#39;ll find instructions on how to complete the technical interview
					pre-requisites for the desired role you&#39;re looking to apply for.
					<br><br>
					Once you have completed the prescribed test, please send back your workings as instructed below.
					One of our consultants will then review and make a decision on whether you will progress to
					the next stage. Rest assured, we&#39;ll of course give you feedback either way.
					<br><br>
					If you&#39;re interested to see more roles at Telstra Purple, make sure to check out
					<a href="http://purple.telstra.com/careers" class="text-red">Telstra Purple Careers</a>.
					<br><br>
					To understand more about our role levels, check out our
					<a href="http://personas.purple.telstra.com/" class="text-red">Career Frameworks</a> page.
				</p></div>
			<img src="/interview.svg" alt="A person sitting at a desk with a laptop" class="max-h-300"></div>
		<h2 class="py-3 lg:pt-12 lg:pb-2">Interview for</h2>
			<div class="flex flex-col lg:flex-row justify-start flex-wrap"><a href="/cloud"><div class="rounded-lg px-5 py-2 bg-red text-white text-lg mr-4 mb-2 hover:bg-black">Cloud</div>
					</a><a href="/data"><div class="rounded-lg px-5 py-2 bg-red text-white text-lg mr-4 mb-2 hover:bg-black">Data</div>
					</a><a href="/development"><div class="rounded-lg px-5 py-2 bg-red text-white text-lg mr-4 mb-2 hover:bg-black">Development</div>
					</a><a href="/devops"><div class="rounded-lg px-5 py-2 bg-red text-white text-lg mr-4 mb-2 hover:bg-black">DevOps</div>
					</a></div></div></div>
	



<div class="px-12 lg:px-32 max-w-screen bg-gray"><h2 class="pb-4 lg:pb-4 text-black text-xl">Instructions for</h2>
	<div class="flex flex-col lg:flex-row justify-start pb-2 lg:pb-8"><a href="#data-analyst" class="text-red"><div class="rounded-lg px-5 py-2 bg-black text-white text-lg mr-4 mb-2 hover:bg-red">Data Analyst</div>
			</a><a href="#data-engineer" class="text-red"><div class="rounded-lg px-5 py-2 bg-black text-white text-lg mr-4 mb-2 hover:bg-red">Data Engineer</div>
			</a></div></div>
<div class="flex flex-col sm:flex-row px-12 lg:px-32 pt-4 pb-5 lg:pt-14 bg-gray-lighter"><article class="Interview"><!-- HTML_TAG_START --><h1 id="data-analyst"><a aria-hidden="true" tabindex="-1" href="#data-analyst"><span class="icon icon-link"></span></a>Data Analyst</h1>
<h2 id="prerequisites"><a aria-hidden="true" tabindex="-1" href="#prerequisites"><span class="icon icon-link"></span></a>Prerequisites</h2>
<p>In order to complete this test, you will need:</p>
<ol>
<li>Your laptop with <a rel="external" href="https://powerbi.microsoft.com/en-us/downloads/">Power BI Desktop</a> installed.</li>
<li>A public GitHub repo, or a Google/OneDrive where you can share your workings (E.g. Power BI files).</li>
</ol>
<h2 id="scenario"><a aria-hidden="true" tabindex="-1" href="#scenario"><span class="icon icon-link"></span></a>Scenario</h2>
<p>As the Data Analyst for a small fuel company, you have been given two (2) data sources.</p>
<ol>
<li><a rel="external" href="http://www.fuelwatch.wa.gov.au/fuelwatch/fuelWatchRSS">The FuelWatch RSS feed</a></li>
<li><a rel="external" href="/code/discount.xlsx">Discounts Excel file</a></li>
</ol>
<h2 id="instructions"><a aria-hidden="true" tabindex="-1" href="#instructions"><span class="icon icon-link"></span></a>Instructions</h2>
<p>You have been ask to create a Power BI report that combines both sources to answer questions the business may have around its competitors.</p>
<h3 id="modelling"><a aria-hidden="true" tabindex="-1" href="#modelling"><span class="icon icon-link"></span></a>Modelling</h3>
<ol>
<li>In Power BI, connect to the two (2) data sources.</li>
<li>Using Power Query, create transformations as necessary to model the source data as described below.</li>
<li>Create three (3) new dimension tables - <strong>Brand</strong>, <strong>Site</strong> and <strong>SiteFeatures</strong>. They should have the respective attributes below.
<ul>
<li>BrandID, BrandName</li>
<li>SiteID, TradingName, Location, Address, Phone, Latitude, Longitude. Please also add a computed, persisted column named <strong><em>FullAddress</em></strong> which combines Address and Location.</li>
<li>SiteFeatureID, SiteID (From the Site table) and the various site features from the delimited source column site-features.</li>
</ul>
</li>
<li>Create a <strong>Date</strong> table automatically. Ensure that the key is numeric in the format yyyymmdd.</li>
<li>Create a single fact table called FuelPrice. It should have the columns below:
<ul>
<li>FuelPriceID (Identity), BrandID (From the Brand table), SiteID (From the Site table), DateID (From the Date table), Price, DateCreated (Default to current system time) and DateModified (nullable, no default).</li>
</ul>
</li>
<li>Clean the data in Power BI by removing any unnecessary columns not mentioned above.</li>
</ol>
<h3 id="visualisation"><a aria-hidden="true" tabindex="-1" href="#visualisation"><span class="icon icon-link"></span></a>Visualisation</h3>
<ol>
<li>Create visuals using the modelled data that allow the business to answer the following questions.
<ul>
<li>What is the average price of fuel across the region over all time?</li>
<li>On what date and where can they find the cheapest fuel after the discount has been applied? We should be able to compare the full price and the discounted price.</li>
<li>See a list of the top 10 sites with the cheapest fuel price on a given date?</li>
<li>Be able to drill-down into all the details for a the top 10 cheapest fuel stations?</li>
<li>Discover quickly which sites are open 24 hours a day?</li>
</ul>
</li>
</ol>
<h3 id="general"><a aria-hidden="true" tabindex="-1" href="#general"><span class="icon icon-link"></span></a>General</h3>
<ol>
<li>The business executives have also asked for a consistent theme that can be applied to the report and other reports in the future.</li>
<li>The business executives are also interested in tracking historical price changes. Can you produce a strategy for this?</li>
</ol>
<h3 id="report-considerations"><a aria-hidden="true" tabindex="-1" href="#report-considerations"><span class="icon icon-link"></span></a>Report Considerations</h3>
<p>While developing the report, keep the following in mind</p>
<ul>
<li>The FuelWatch data is updated on a daily basis, how would your model manage this?</li>
<li>Good use of dimensional modelling.</li>
<li>Use of efficient DAX/M.</li>
<li>At least two different types of visuals and extra elements (like text boxes, shapes etc.)</li>
<li>A clean user friendly report that’s easy to read, visually appealing and answers the questions.</li>
</ul>
<h2 id="completion"><a aria-hidden="true" tabindex="-1" href="#completion"><span class="icon icon-link"></span></a>Completion</h2>
<p>When the solution is above is complete, create a ReadMe document describing how your model manages new data on subsequent days and then save this with the Power BI file and any other supporting templates and share with the Consultant/Talent Acquisition Specialist who contacted you from Telstra Purple.</p>
<h1 id="data-engineer"><a aria-hidden="true" tabindex="-1" href="#data-engineer"><span class="icon icon-link"></span></a>Data Engineer</h1>
<h2 id="prerequisites-1"><a aria-hidden="true" tabindex="-1" href="#prerequisites-1"><span class="icon icon-link"></span></a>Prerequisites</h2>
<p>In order to complete this test, you will need:</p>
<ol>
<li>Access to a Cloud service provider environment.
<ul>
<li>For an Azure Subscription, you can create a <a rel="external" href="https://azure.microsoft.com/en-au/free/">30-day free trial</a> if you don't have an Azure subscription.</li>
<li>For an AWS Account, you can utilise <a rel="external" href="https://aws.amazon.com/free/">AWS free tier</a> if you don't have an AWS Account.</li>
<li>For an GCP Environment, you can utilise <a rel="external" href="https://cloud.google.com/free">GCP free tier</a> if you don't have an GCP Account.</li>
</ul>
</li>
<li>Your laptop with the tools/IDE that you like to work with.</li>
<li>A public GitHub repo, or a Google/OneDrive where you can share your solution.</li>
</ol>
<h2 id="scenario-1"><a aria-hidden="true" tabindex="-1" href="#scenario-1"><span class="icon icon-link"></span></a>Scenario</h2>
<p>We are assisting a customer in building out a solution which ingests the latest fuel prices from the Fuel Watch RSS feed. This data is not in a great structure, and we would like to ingest this data into a data lake, perform some transformations and build out a data mart for use in Power BI, the customer's visualisation tool of choice.</p>
<ol>
<li>Our customer would like to take an infrastructure-as-code approach to the deployment of the new resources however, the delivery of a working prototype is the highest priority.</li>
<li>All resources should be deployed in Australia.</li>
</ol>
<h2 id="instructions-1"><a aria-hidden="true" tabindex="-1" href="#instructions-1"><span class="icon icon-link"></span></a>Instructions</h2>
<p>Follow these instructions and deploy using whatever means you feel necessary.</p>
<h3 id="building-the-platform"><a aria-hidden="true" tabindex="-1" href="#building-the-platform"><span class="icon icon-link"></span></a>Building the Platform</h3>
<p>First, create your resources in your Cloud service provider environment.</p>
<ol>
<li>Create all resources with the following tags.
<ul>
<li>Department - Finance</li>
<li>Environment - Development</li>
</ul>
</li>
<li>Create a new storage resource.
<ul>
<li>Add a container/bucket called <strong>datalakestore</strong>.</li>
</ul>
</li>
<li>Create the following ETL resource applicable to your Cloud service provider environment.
<ul>
<li>Azure - Azure Data Factory</li>
<li>AWS - Glue Data Pipeline</li>
<li>GCP - Cloud Data Fusion</li>
</ul>
</li>
<li>Create the following encryption resource to your Cloud service provider environment.
<ul>
<li>Azure - Azure Key Vault</li>
<li>AWS - Key Management Service</li>
<li>GCP - Secret Manager</li>
</ul>
</li>
<li>Create a new SQL Server database called <strong>DataMart</strong>.
<ul>
<li>The database should be a PaaS version.</li>
<li>The database size should be 20GB.</li>
<li>Authentication should use contained users.</li>
</ul>
</li>
</ol>
<h3 id="deployability"><a aria-hidden="true" tabindex="-1" href="#deployability"><span class="icon icon-link"></span></a>Deployability</h3>
<p>Ensure that your resources can be deployed in a repeatable manner and can be parameterised so that e.g. Dev/Test/Prod copies can all be deployed from the same IaaS source code.
Save your deployment code artefacts for upload with the remainder of your solution.</p>
<h3 id="ingestion"><a aria-hidden="true" tabindex="-1" href="#ingestion"><span class="icon icon-link"></span></a>Ingestion</h3>
<p>The source data you will be using is the <a rel="external" href="http://www.fuelwatch.wa.gov.au/fuelwatch/fuelWatchRSS">FuelWatch RSS feed</a> from the Western Australian Government.</p>
<ol>
<li>Any passwords, keys or other secrets used in your solution (e.g. storage keys, database passwords, etc) should be saved in your deployed encryption resource and referenced from there.</li>
<li>Ingest the RSS feed into the <strong>datalakestore</strong> container/bucket in the storage account deployed above.</li>
<li>Import the <strong>item</strong> array from the RSS object, with the child fields in a tabular format, into a parquet file in the storage account with the path below.
<ul>
<li>datalakestore/Raw/FuelWatch/&#x3C;Current date as format yyyy-mm-dd>.</li>
<li>The file name should be <strong>feed.parquet</strong>.</li>
</ul>
</li>
<li>Once the file has been imported into the data lake, add another activity to the same pipeline which reads the parquet file from the data lake and writes it to the DataMart SQL database.
<ul>
<li>Use a temporary table in the tempstage schema called <strong>FuelPrices</strong>. This activity must run after the activity to import the source data.</li>
<li>The connection to the SQL PaaS database should be secured and not exposed in your pipeline code.</li>
</ul>
</li>
</ol>
<h3 id="modelling-1"><a aria-hidden="true" tabindex="-1" href="#modelling-1"><span class="icon icon-link"></span></a>Modelling</h3>
<p>Once the temporary data has been loaded into the DataMart database, transform the data into fact and dimension tables for our user-consumed data model. All objects should be created in a new schema named <strong>dw</strong>.</p>
<ol>
<li>Create three (3) new dimension tables - <strong>Brand</strong>, <strong>Site</strong> and <strong>SiteFeatures</strong>. They should have the respective attributes below.
<ul>
<li>BrandID (Identity, primary key), BrandName</li>
<li>SiteID (Identity, primary key), TradingName, Location, Address, Phone, Latitude, Longitude. Add a computed, persisted column named FullAddress which combines Address and Location.</li>
<li>SiteFeatureID (Identity, primary key), SiteID (From the Site table) and the various site features from the delimited source column site-features.</li>
<li>Create Clustered Indexes on the Primary Keys and Non-Clustered Indexes on any Foreign Keys.</li>
</ul>
</li>
<li>Create a single fact table called <strong>FuelPrice</strong>. It should have the columns below:
<ul>
<li>FuelPriceID (Identity), BrandID (From the Brand table), SiteID (From the Site table), Price, DateCreated (Numeric and stored as yyyymmdd. Default to current date) and DateModified (Numeric and stored as yyyymmdd, nullable, no default).</li>
</ul>
</li>
<li>Add a primary key to the <strong>FuelPrice</strong> fact table across the SiteID, BrandID and DateCreated columns
<ul>
<li>Add a clustered index on the FuelPriceID column.</li>
</ul>
</li>
<li>The naming convention for indexes should be <code>IDX_&#x3C;Table Name>_&#x3C;Column Name></code>.</li>
<li>Create appropriate artefacts, scripts or pipelines to populate the tables above.
<ul>
<li>The pipeline must be able to handle intra-day changes to the prices by modifying the current days' record.</li>
<li>The pipeline must be able to handle subsequent days' prices by adding new records.</li>
</ul>
</li>
<li>Keep these artefacts handy for the technical interview.</li>
</ol>
<h2 id="completion-1"><a aria-hidden="true" tabindex="-1" href="#completion-1"><span class="icon icon-link"></span></a>Completion</h2>
<p>When the solution above is complete, save any artefacts created and share them with the Consultant/Talent Acquisition Specialist who contacted you from Telstra Purple. We should be able to define appropriate parameters and deploy your solution to our own Cloud service provider environment and be able to execute your pipelines to populate the tables.</p>
<p>If also possible, please keep your Cloud service provider environment available (feel free to pause or scale-down any resources to save costs) until the in-person technical interview has been conducted or as otherwise advised.</p><!-- HTML_TAG_END --></article></div>
<button id="scrollToTopButton" class="fixed bottom-5 right-7 bg-red text-white p-3 text-lg rounded-xl hidden">Top</button></div>



			<script type="application/json" data-type="svelte-data" data-url="/index.json">{"status":200,"statusText":"","headers":{"content-type":"text/plain;charset=UTF-8"},"body":"[{\"metadata\":{\"title\":\"Cloud\",\"interviews\":\"Azure Cloud,AWS Cloud\",\"sortorder\":1},\"slug\":\"cloud\"},{\"metadata\":{\"title\":\"Data\",\"interviews\":\"Data Analyst,Data Engineer\",\"sortorder\":2},\"slug\":\"data\"},{\"metadata\":{\"title\":\"Development\",\"interviews\":\"Toy Robot\",\"sortorder\":3},\"slug\":\"development\"},{\"metadata\":{\"title\":\"DevOps\",\"interviews\":\"Automation Specialist\",\"sortorder\":4},\"slug\":\"devops\"}]"}</script>

	<script type="application/json" data-type="svelte-data" data-url="/data.json">{"status":200,"statusText":"","headers":{"content-type":"text/plain;charset=UTF-8"},"body":"{\"metadata\":{\"title\":\"Data\",\"interviews\":\"Data Analyst,Data Engineer\",\"sortorder\":2},\"content\":\"\u003Ch1 id=\\\"data-analyst\\\"\u003E\u003Ca aria-hidden=\\\"true\\\" tabindex=\\\"-1\\\" href=\\\"#data-analyst\\\"\u003E\u003Cspan class=\\\"icon icon-link\\\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EData Analyst\u003C\u002Fh1\u003E\\n\u003Ch2 id=\\\"prerequisites\\\"\u003E\u003Ca aria-hidden=\\\"true\\\" tabindex=\\\"-1\\\" href=\\\"#prerequisites\\\"\u003E\u003Cspan class=\\\"icon icon-link\\\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EPrerequisites\u003C\u002Fh2\u003E\\n\u003Cp\u003EIn order to complete this test, you will need:\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003EYour laptop with \u003Ca href=\\\"https:\u002F\u002Fpowerbi.microsoft.com\u002Fen-us\u002Fdownloads\u002F\\\"\u003EPower BI Desktop\u003C\u002Fa\u003E installed.\u003C\u002Fli\u003E\\n\u003Cli\u003EA public GitHub repo, or a Google\u002FOneDrive where you can share your workings (E.g. Power BI files).\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Ch2 id=\\\"scenario\\\"\u003E\u003Ca aria-hidden=\\\"true\\\" tabindex=\\\"-1\\\" href=\\\"#scenario\\\"\u003E\u003Cspan class=\\\"icon icon-link\\\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EScenario\u003C\u002Fh2\u003E\\n\u003Cp\u003EAs the Data Analyst for a small fuel company, you have been given two (2) data sources.\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003E\u003Ca href=\\\"http:\u002F\u002Fwww.fuelwatch.wa.gov.au\u002Ffuelwatch\u002FfuelWatchRSS\\\"\u003EThe FuelWatch RSS feed\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ca href=\\\"\u002Fcode\u002Fdiscount.xlsx\\\"\u003EDiscounts Excel file\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Ch2 id=\\\"instructions\\\"\u003E\u003Ca aria-hidden=\\\"true\\\" tabindex=\\\"-1\\\" href=\\\"#instructions\\\"\u003E\u003Cspan class=\\\"icon icon-link\\\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EInstructions\u003C\u002Fh2\u003E\\n\u003Cp\u003EYou have been ask to create a Power BI report that combines both sources to answer questions the business may have around its competitors.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"modelling\\\"\u003E\u003Ca aria-hidden=\\\"true\\\" tabindex=\\\"-1\\\" href=\\\"#modelling\\\"\u003E\u003Cspan class=\\\"icon icon-link\\\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EModelling\u003C\u002Fh3\u003E\\n\u003Col\u003E\\n\u003Cli\u003EIn Power BI, connect to the two (2) data sources.\u003C\u002Fli\u003E\\n\u003Cli\u003EUsing Power Query, create transformations as necessary to model the source data as described below.\u003C\u002Fli\u003E\\n\u003Cli\u003ECreate three (3) new dimension tables - \u003Cstrong\u003EBrand\u003C\u002Fstrong\u003E, \u003Cstrong\u003ESite\u003C\u002Fstrong\u003E and \u003Cstrong\u003ESiteFeatures\u003C\u002Fstrong\u003E. They should have the respective attributes below.\\n\u003Cul\u003E\\n\u003Cli\u003EBrandID, BrandName\u003C\u002Fli\u003E\\n\u003Cli\u003ESiteID, TradingName, Location, Address, Phone, Latitude, Longitude. Please also add a computed, persisted column named \u003Cstrong\u003E\u003Cem\u003EFullAddress\u003C\u002Fem\u003E\u003C\u002Fstrong\u003E which combines Address and Location.\u003C\u002Fli\u003E\\n\u003Cli\u003ESiteFeatureID, SiteID (From the Site table) and the various site features from the delimited source column site-features.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003ECreate a \u003Cstrong\u003EDate\u003C\u002Fstrong\u003E table automatically. Ensure that the key is numeric in the format yyyymmdd.\u003C\u002Fli\u003E\\n\u003Cli\u003ECreate a single fact table called FuelPrice. It should have the columns below:\\n\u003Cul\u003E\\n\u003Cli\u003EFuelPriceID (Identity), BrandID (From the Brand table), SiteID (From the Site table), DateID (From the Date table), Price, DateCreated (Default to current system time) and DateModified (nullable, no default).\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003EClean the data in Power BI by removing any unnecessary columns not mentioned above.\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Ch3 id=\\\"visualisation\\\"\u003E\u003Ca aria-hidden=\\\"true\\\" tabindex=\\\"-1\\\" href=\\\"#visualisation\\\"\u003E\u003Cspan class=\\\"icon icon-link\\\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EVisualisation\u003C\u002Fh3\u003E\\n\u003Col\u003E\\n\u003Cli\u003ECreate visuals using the modelled data that allow the business to answer the following questions.\\n\u003Cul\u003E\\n\u003Cli\u003EWhat is the average price of fuel across the region over all time?\u003C\u002Fli\u003E\\n\u003Cli\u003EOn what date and where can they find the cheapest fuel after the discount has been applied? We should be able to compare the full price and the discounted price.\u003C\u002Fli\u003E\\n\u003Cli\u003ESee a list of the top 10 sites with the cheapest fuel price on a given date?\u003C\u002Fli\u003E\\n\u003Cli\u003EBe able to drill-down into all the details for a the top 10 cheapest fuel stations?\u003C\u002Fli\u003E\\n\u003Cli\u003EDiscover quickly which sites are open 24 hours a day?\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Ch3 id=\\\"general\\\"\u003E\u003Ca aria-hidden=\\\"true\\\" tabindex=\\\"-1\\\" href=\\\"#general\\\"\u003E\u003Cspan class=\\\"icon icon-link\\\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EGeneral\u003C\u002Fh3\u003E\\n\u003Col\u003E\\n\u003Cli\u003EThe business executives have also asked for a consistent theme that can be applied to the report and other reports in the future.\u003C\u002Fli\u003E\\n\u003Cli\u003EThe business executives are also interested in tracking historical price changes. Can you produce a strategy for this?\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Ch3 id=\\\"report-considerations\\\"\u003E\u003Ca aria-hidden=\\\"true\\\" tabindex=\\\"-1\\\" href=\\\"#report-considerations\\\"\u003E\u003Cspan class=\\\"icon icon-link\\\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EReport Considerations\u003C\u002Fh3\u003E\\n\u003Cp\u003EWhile developing the report, keep the following in mind\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003EThe FuelWatch data is updated on a daily basis, how would your model manage this?\u003C\u002Fli\u003E\\n\u003Cli\u003EGood use of dimensional modelling.\u003C\u002Fli\u003E\\n\u003Cli\u003EUse of efficient DAX\u002FM.\u003C\u002Fli\u003E\\n\u003Cli\u003EAt least two different types of visuals and extra elements (like text boxes, shapes etc.)\u003C\u002Fli\u003E\\n\u003Cli\u003EA clean user friendly report that’s easy to read, visually appealing and answers the questions.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Ch2 id=\\\"completion\\\"\u003E\u003Ca aria-hidden=\\\"true\\\" tabindex=\\\"-1\\\" href=\\\"#completion\\\"\u003E\u003Cspan class=\\\"icon icon-link\\\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003ECompletion\u003C\u002Fh2\u003E\\n\u003Cp\u003EWhen the solution is above is complete, create a ReadMe document describing how your model manages new data on subsequent days and then save this with the Power BI file and any other supporting templates and share with the Consultant\u002FTalent Acquisition Specialist who contacted you from Telstra Purple.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"data-engineer\\\"\u003E\u003Ca aria-hidden=\\\"true\\\" tabindex=\\\"-1\\\" href=\\\"#data-engineer\\\"\u003E\u003Cspan class=\\\"icon icon-link\\\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EData Engineer\u003C\u002Fh1\u003E\\n\u003Ch2 id=\\\"prerequisites-1\\\"\u003E\u003Ca aria-hidden=\\\"true\\\" tabindex=\\\"-1\\\" href=\\\"#prerequisites-1\\\"\u003E\u003Cspan class=\\\"icon icon-link\\\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EPrerequisites\u003C\u002Fh2\u003E\\n\u003Cp\u003EIn order to complete this test, you will need:\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003EAccess to a Cloud service provider environment.\\n\u003Cul\u003E\\n\u003Cli\u003EFor an Azure Subscription, you can create a \u003Ca href=\\\"https:\u002F\u002Fazure.microsoft.com\u002Fen-au\u002Ffree\u002F\\\"\u003E30-day free trial\u003C\u002Fa\u003E if you don't have an Azure subscription.\u003C\u002Fli\u003E\\n\u003Cli\u003EFor an AWS Account, you can utilise \u003Ca href=\\\"https:\u002F\u002Faws.amazon.com\u002Ffree\u002F\\\"\u003EAWS free tier\u003C\u002Fa\u003E if you don't have an AWS Account.\u003C\u002Fli\u003E\\n\u003Cli\u003EFor an GCP Environment, you can utilise \u003Ca href=\\\"https:\u002F\u002Fcloud.google.com\u002Ffree\\\"\u003EGCP free tier\u003C\u002Fa\u003E if you don't have an GCP Account.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003EYour laptop with the tools\u002FIDE that you like to work with.\u003C\u002Fli\u003E\\n\u003Cli\u003EA public GitHub repo, or a Google\u002FOneDrive where you can share your solution.\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Ch2 id=\\\"scenario-1\\\"\u003E\u003Ca aria-hidden=\\\"true\\\" tabindex=\\\"-1\\\" href=\\\"#scenario-1\\\"\u003E\u003Cspan class=\\\"icon icon-link\\\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EScenario\u003C\u002Fh2\u003E\\n\u003Cp\u003EWe are assisting a customer in building out a solution which ingests the latest fuel prices from the Fuel Watch RSS feed. This data is not in a great structure, and we would like to ingest this data into a data lake, perform some transformations and build out a data mart for use in Power BI, the customer's visualisation tool of choice.\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003EOur customer would like to take an infrastructure-as-code approach to the deployment of the new resources however, the delivery of a working prototype is the highest priority.\u003C\u002Fli\u003E\\n\u003Cli\u003EAll resources should be deployed in Australia.\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Ch2 id=\\\"instructions-1\\\"\u003E\u003Ca aria-hidden=\\\"true\\\" tabindex=\\\"-1\\\" href=\\\"#instructions-1\\\"\u003E\u003Cspan class=\\\"icon icon-link\\\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EInstructions\u003C\u002Fh2\u003E\\n\u003Cp\u003EFollow these instructions and deploy using whatever means you feel necessary.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"building-the-platform\\\"\u003E\u003Ca aria-hidden=\\\"true\\\" tabindex=\\\"-1\\\" href=\\\"#building-the-platform\\\"\u003E\u003Cspan class=\\\"icon icon-link\\\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EBuilding the Platform\u003C\u002Fh3\u003E\\n\u003Cp\u003EFirst, create your resources in your Cloud service provider environment.\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003ECreate all resources with the following tags.\\n\u003Cul\u003E\\n\u003Cli\u003EDepartment - Finance\u003C\u002Fli\u003E\\n\u003Cli\u003EEnvironment - Development\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003ECreate a new storage resource.\\n\u003Cul\u003E\\n\u003Cli\u003EAdd a container\u002Fbucket called \u003Cstrong\u003Edatalakestore\u003C\u002Fstrong\u003E.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003ECreate the following ETL resource applicable to your Cloud service provider environment.\\n\u003Cul\u003E\\n\u003Cli\u003EAzure - Azure Data Factory\u003C\u002Fli\u003E\\n\u003Cli\u003EAWS - Glue Data Pipeline\u003C\u002Fli\u003E\\n\u003Cli\u003EGCP - Cloud Data Fusion\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003ECreate the following encryption resource to your Cloud service provider environment.\\n\u003Cul\u003E\\n\u003Cli\u003EAzure - Azure Key Vault\u003C\u002Fli\u003E\\n\u003Cli\u003EAWS - Key Management Service\u003C\u002Fli\u003E\\n\u003Cli\u003EGCP - Secret Manager\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003ECreate a new SQL Server database called \u003Cstrong\u003EDataMart\u003C\u002Fstrong\u003E.\\n\u003Cul\u003E\\n\u003Cli\u003EThe database should be a PaaS version.\u003C\u002Fli\u003E\\n\u003Cli\u003EThe database size should be 20GB.\u003C\u002Fli\u003E\\n\u003Cli\u003EAuthentication should use contained users.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Ch3 id=\\\"deployability\\\"\u003E\u003Ca aria-hidden=\\\"true\\\" tabindex=\\\"-1\\\" href=\\\"#deployability\\\"\u003E\u003Cspan class=\\\"icon icon-link\\\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EDeployability\u003C\u002Fh3\u003E\\n\u003Cp\u003EEnsure that your resources can be deployed in a repeatable manner and can be parameterised so that e.g. Dev\u002FTest\u002FProd copies can all be deployed from the same IaaS source code.\\nSave your deployment code artefacts for upload with the remainder of your solution.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"ingestion\\\"\u003E\u003Ca aria-hidden=\\\"true\\\" tabindex=\\\"-1\\\" href=\\\"#ingestion\\\"\u003E\u003Cspan class=\\\"icon icon-link\\\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EIngestion\u003C\u002Fh3\u003E\\n\u003Cp\u003EThe source data you will be using is the \u003Ca href=\\\"http:\u002F\u002Fwww.fuelwatch.wa.gov.au\u002Ffuelwatch\u002FfuelWatchRSS\\\"\u003EFuelWatch RSS feed\u003C\u002Fa\u003E from the Western Australian Government.\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003EAny passwords, keys or other secrets used in your solution (e.g. storage keys, database passwords, etc) should be saved in your deployed encryption resource and referenced from there.\u003C\u002Fli\u003E\\n\u003Cli\u003EIngest the RSS feed into the \u003Cstrong\u003Edatalakestore\u003C\u002Fstrong\u003E container\u002Fbucket in the storage account deployed above.\u003C\u002Fli\u003E\\n\u003Cli\u003EImport the \u003Cstrong\u003Eitem\u003C\u002Fstrong\u003E array from the RSS object, with the child fields in a tabular format, into a parquet file in the storage account with the path below.\\n\u003Cul\u003E\\n\u003Cli\u003Edatalakestore\u002FRaw\u002FFuelWatch\u002F&#x3C;Current date as format yyyy-mm-dd\u003E.\u003C\u002Fli\u003E\\n\u003Cli\u003EThe file name should be \u003Cstrong\u003Efeed.parquet\u003C\u002Fstrong\u003E.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003EOnce the file has been imported into the data lake, add another activity to the same pipeline which reads the parquet file from the data lake and writes it to the DataMart SQL database.\\n\u003Cul\u003E\\n\u003Cli\u003EUse a temporary table in the tempstage schema called \u003Cstrong\u003EFuelPrices\u003C\u002Fstrong\u003E. This activity must run after the activity to import the source data.\u003C\u002Fli\u003E\\n\u003Cli\u003EThe connection to the SQL PaaS database should be secured and not exposed in your pipeline code.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Ch3 id=\\\"modelling-1\\\"\u003E\u003Ca aria-hidden=\\\"true\\\" tabindex=\\\"-1\\\" href=\\\"#modelling-1\\\"\u003E\u003Cspan class=\\\"icon icon-link\\\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EModelling\u003C\u002Fh3\u003E\\n\u003Cp\u003EOnce the temporary data has been loaded into the DataMart database, transform the data into fact and dimension tables for our user-consumed data model. All objects should be created in a new schema named \u003Cstrong\u003Edw\u003C\u002Fstrong\u003E.\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003ECreate three (3) new dimension tables - \u003Cstrong\u003EBrand\u003C\u002Fstrong\u003E, \u003Cstrong\u003ESite\u003C\u002Fstrong\u003E and \u003Cstrong\u003ESiteFeatures\u003C\u002Fstrong\u003E. They should have the respective attributes below.\\n\u003Cul\u003E\\n\u003Cli\u003EBrandID (Identity, primary key), BrandName\u003C\u002Fli\u003E\\n\u003Cli\u003ESiteID (Identity, primary key), TradingName, Location, Address, Phone, Latitude, Longitude. Add a computed, persisted column named FullAddress which combines Address and Location.\u003C\u002Fli\u003E\\n\u003Cli\u003ESiteFeatureID (Identity, primary key), SiteID (From the Site table) and the various site features from the delimited source column site-features.\u003C\u002Fli\u003E\\n\u003Cli\u003ECreate Clustered Indexes on the Primary Keys and Non-Clustered Indexes on any Foreign Keys.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003ECreate a single fact table called \u003Cstrong\u003EFuelPrice\u003C\u002Fstrong\u003E. It should have the columns below:\\n\u003Cul\u003E\\n\u003Cli\u003EFuelPriceID (Identity), BrandID (From the Brand table), SiteID (From the Site table), Price, DateCreated (Numeric and stored as yyyymmdd. Default to current date) and DateModified (Numeric and stored as yyyymmdd, nullable, no default).\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003EAdd a primary key to the \u003Cstrong\u003EFuelPrice\u003C\u002Fstrong\u003E fact table across the SiteID, BrandID and DateCreated columns\\n\u003Cul\u003E\\n\u003Cli\u003EAdd a clustered index on the FuelPriceID column.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003EThe naming convention for indexes should be \u003Ccode\u003EIDX_&#x3C;Table Name\u003E_&#x3C;Column Name\u003E\u003C\u002Fcode\u003E.\u003C\u002Fli\u003E\\n\u003Cli\u003ECreate appropriate artefacts, scripts or pipelines to populate the tables above.\\n\u003Cul\u003E\\n\u003Cli\u003EThe pipeline must be able to handle intra-day changes to the prices by modifying the current days' record.\u003C\u002Fli\u003E\\n\u003Cli\u003EThe pipeline must be able to handle subsequent days' prices by adding new records.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003EKeep these artefacts handy for the technical interview.\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Ch2 id=\\\"completion-1\\\"\u003E\u003Ca aria-hidden=\\\"true\\\" tabindex=\\\"-1\\\" href=\\\"#completion-1\\\"\u003E\u003Cspan class=\\\"icon icon-link\\\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003ECompletion\u003C\u002Fh2\u003E\\n\u003Cp\u003EWhen the solution above is complete, save any artefacts created and share them with the Consultant\u002FTalent Acquisition Specialist who contacted you from Telstra Purple. We should be able to define appropriate parameters and deploy your solution to our own Cloud service provider environment and be able to execute your pipelines to populate the tables.\u003C\u002Fp\u003E\\n\u003Cp\u003EIf also possible, please keep your Cloud service provider environment available (feel free to pause or scale-down any resources to save costs) until the in-person technical interview has been conducted or as otherwise advised.\u003C\u002Fp\u003E\"}"}</script>
		</div>
	</body>
</html>
